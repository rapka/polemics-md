# Superintelligence: The Idea That's Eating Smart People

In 2016, entrepeneur and tech writer Maciej Ceglowski published a talk/essay called ]Superintelligence: The Idea That Eats Smart People](https://idlewords.com/talks/superintelligence.htm) about the concept of "superintelligent" AI. This article is quite possibly the most prescient pieces of writing on the topic of AI in recent memory, as if the Greek God Apollo has seen fit to gift Mr. Ceglowski the gift of prophecy. Unfortunately, the topic of this particular prophecy tends to make ones' head hurt, much like a dodgeball. 

https://www.sfgate.com/news/bayarea/article/Man-Gets-5-Years-For-Attacking-Woman-Outside-13796663.php

[tumblr meme goes here]

In light of OpenAI's announcement of their [Superalignment](https://openai.com/blog/introducing-superalignment) project, I thought I'd revist this article and see how well it holds up in 2023. I'll highlight some of the portions from Ceglowski's essay that are most relevant to the AI debate today, but I encourage readers to read the full essay on Idle Words' blog.
___

Ceglowski's piece is largely a response to the Superintellgent AI conceived by Nick Bostrom in his influential 2015 book Superintelligence.

> Last year, the philosopher Nick Bostrom published Superintelligence, a book that synthesizes the alarmist view of AI and makes a case that such an intelligence explosion is both dangerous and inevitable given a set of modest assumptions. 
> The computer that takes over the world is a staple scifi trope. But enough people take this scenario seriously that we have to take them seriously. Stephen Hawking, Elon Musk, and a whole raft of Silicon Valley investors and billionaires find this argument persuasive. 

It is now very clear in 2023 that we do not have to and should not take Elon Musk seriously about anything, but there's still many other smart people who are persuaded by the narratives of Nick Bostrom and his ilk. Concerns about existential risks from AI have only been amplified since the creation of [large language models](https://en.wikipedia.org/wiki/Large_language_model) like OpenAI's ChatGPT and Google Bard. This group of people also includes many current and former Google and OpenAI employees who have been [vocal](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html) about these concerns. I don't think anything that has happened since 2016 makes these concerns any more based in reality now, but I do believe increased visibility and reach of AI risk narratives needs to be challenged for the same reasons Ceglowski has laid out in his essay

Readers should note that Nick Bostrom was embroiled in a [controversy](https://www.vice.com/en/article/z34dm3/prominent-ai-philosopher-and-father-of-longtermism-sent-very-racist-email-to-a-90s-philosophy-listserv) in early 2023 regarding his beliefs about racial IQ differences and subsequent use of the n-word to describe those beliefs. These beliefs were originally expressed in an email chain in the 1990's and were unearthed in January, leading to Bostrom releasing a [defensive half-apology](https://nickbostrom.com/oldemail.pdf) in which he tries to explain his "actual views" and say "leaves it to others" to debate whether genetics play a factor in racial cognitive differences. As Vice points out in their article, this debate has continued to resurface since Charles Murray's 1994 book The Bell Curve and has rightfully been described as racist pseudo-science since then.

Naturally, none of this means that his ideas about AI are necessarily wrong, but it is very concerning that this behavior is coming from someone who wants to have a say in in the direction our entire species takes. The [Future of Humanity Institute](https://www.fhi.ox.ac.uk/about-fhi/) at Oxford University, which Bostrom founded and leads, "works on big picture questions for human civilisation and explores what can be done now to ensure a flourishing long-term future". It's hard to imagine that someone who who displays such sentiments about people who make up a large percent of human civilization today.

Ceglowski goes on to summarize Bostrom's imagined doomsday scenario, substituting the oft-used ["paperclip maximizer"](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer) thought expirement with a hypothetical superintelligent joke-writing robot. This scenario culminates in an apocalyptic manner as the AI continually self-improves to the point of being able to kill people via laughter. 

> When the researchers come in on Monday, the AI has become tens of thousands of times funnier than any human being who ever lived. It greets them with a joke, and they die laughing. 
> In fact, anyone who tries to communicate with the robot dies laughing, just like in the Monty Python skit. The human species laughs itself into extinction. 
>Finally, once it's destroyed humanity, the AI builds spaceships and nanorockets to explore the farthest reaches of the galaxy, and find other species to amuse. 

This hypothetical scenario is not too far off from some of the most hyperbolic descriptions of the emergent behaviors some argue LLMs are capable of.

> In 1945, as American physicists were preparing to test the atomic bomb, it occurred to someone to ask if such a test could set the atmosphere on fire. 

> Today we're building another world-changing technology, machine intelligence. We know that it will affect the world in profound ways, change how the economy works, and have knock-on effects we can't predict. 

The invocation of the atomic bomb here should be familiar to anyone who's been online in the past year. The hype behind recent advancements in machine learning combined with Christopher Nolan's wildly popular Oppenheimer biopic makes comparisons between AI and The Manhattan project. I've even used the analogy myself, but as time goes on I've become increasingly worried about how such comparisons frame the potential risks and benefits of machine learning research in near-apocalyptic terms and the effect this can have on culture and poltics. The nature of splitting the atom and its associated dangers have been thoughly explored by physicists for decades through demonstrate expirements, sometime at the expense of their health. The existential dangers of AI on the other hand exist solely as a fantasy scenario presented by think tanks.

After descibing this possible doomsday scenario, Ceglowski's main thesis is presented:
>This scenario is a caricature of Bostrom's argument, because I am not trying to convince you of it, but vaccinate you against it. 

Given that amount of attention still being given to this topic in 2023, it's clear to me that vaccination against this rhetoric is still sorely needed. Despite what OpenAI might have us believe, the greatest risk in machine learning right now isn't existential risk from the technology, it's the risk 

Ceglowski spends the next section describing the vivid language used by AI alarmists such as Bostrom and Elizer Yudkowsky in their fantasies. 

>  There's a lot of vivid language around such a takeover would happen. Nick Bostrom imagines a scenario where a program has become sentient, is biding its time, and has secretly built little DNA replicators. Then, when it's ready: 
>> Nanofactories producing nerve gas or target-seeking mosquito-like missiles might burgeon forth simultaneously from every square meter of the globe. And that will be the end of humanity. 

> Here's a very poetic example from Eliezer Yudkowsky of the good old American values we're supposed to be teaching to our artificial intelligence: 
>> Coherent Extrapolated Volition (CEV) is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted. 

> How's that for a design document? Now go write the code. 
> 
> Hopefully you see the resemblance between this vision of AI and a genie from folklore. The AI is all-powerful and gives you what you ask for, but interprets everything in a super-literal way that you end up regretting. 

Like Bostrom, Eliezer Yudkowsky has been involved in a non-insignificant amount of controversy in the years since this article was published. These controversies are too extensive to include in this article, but at the root of some of them is the fact that Yudkowsky [does not have any formal computer science training](https://www.cnbc.com/id/48538963) or even a high school degree. This is reflected by the lack of detail in his description of "Coherent Extrapolated Volition" and helps explain why he conceives of AI like a genie from folklore. Despite this, his writings and viewed have managed to be widespread enough that they directly contributed to the creation of OpenAI and DeepMind (now owned by Google).

(NYT quote/link here)






(Author) continues to make some very apt cultural comparisons 
 
 > When I was in my twenties, I lived in Vermont, a remote, rural state. Many times I would return from some business trip on an evening flight, and have to drive home for an hour through the dark forest.
> I would listen to a late-night radio program hosted by Art Bell, who had an all-night talk show and would interview various conspiracy theorists and fringe thinkers.
> I would arrive at home totally freaked out, or pull over under a streetlight, convinced that a UFO was about to abduct me. I learned that I am an incredibly persuadable person.

> It's the same feeling I get when I read these AI scenarios. 
> So I was delighted some years later to come across an essay by Scott Alexander about what he calls epistemic learned helplessness. 
> Epistemology is one of those big words, but all it means is "how do you know what you know is true?". Alexander noticed that when he was a young man, he would be taken in by "alternative" histories he read by various crackpots. He would read the history and be utterly convinced, then read the rebuttal and be convinced by that, and so on. 
> At some point he noticed was these alternative histories were mutually contradictory, so they could not possibly all be true. And from that he reasoned that he was simply somebody who could not trust his judgement. He was too easily persuaded. 

I'll be exploring how rural Vermont and fringe thinking are particularly relevant to this subject in a future article. 

Unfortunately, this Scott Alexander quote is one of the point where this essay kind of misses the mark as it turns out that Scott Alexander is ironically one of these crackpots himself. He has been "taken in" by the Neo-Nazi "Human biodiversity" idea, as revealed by leaked emails from 2014

[hbd emails ehere.



He's burying the lede on Scott Alexander](nyt hbd link) here, as he's awful in his own right, but i think this quote illustrates how there have been differing opinions on the superintelligent AI "issue" within different thinkers of the ideologies sometimes collectively reffered to as "TESCREAL". Within the acronym, Yudkowsky is responsible for coining the "Rationality" (R) while Alexander is most often associated with the Effective Altruism movement (EA). Like Yudkowsky, there's far too much to be said about him that can fit into this article. Ironically "various crackpots" is an apt way of descrbing the vast majority of these theorists.
 
 
>People who believe in superintelligence present an interesting case, because many of them are freakishly smart. They can argue you into the ground. But are their arguments right, or is there just something about very smart minds that leaves them vulnerable to religious conversion about AI risk, and makes them particularly persuasive?

>Is the idea of "superintelligence" just a memetic hazard? 
Time has shown the answer to be an unequivocal "yes". 

Over the next few articles, I'll be explaining just how hazardous this idea is, how all these individuals play into it, and how much money is now tied up in it.

>When you're evaluating persuasive arguments about something strange, there are two perspectives you can choose, the inside one or the outside one. 
>Say that some people show up at your front door one day wearing funny robes, asking you if you will join their movement. They believe that a UFO is going to visit Earth two years from now, and it is our task to prepare humanity for the Great Upbeaming. 

>But the outside view tells you something different. These people are wearing funny robes and beads, they live in a remote compound, and they speak in unison in a really creepy way. Even though their arguments are irrefutable, everything in your experience tells you you're dealing with a cult. 

This is the most prophetic park of Ceglowski's argument. Not [only is there a real AI risk compound](atlantic link) where people wear robes, but it also is located in rural Vermont! This group is called MAPLE and I'll be exploring them in depth in a future piece. For the time being, I think this comparison of the illustration from the Superintelligence essay and the photos from the Atlantic's piece speaks volumes




Boy oh boy was he right about the robes and the remote compound...

[img of glowing eye from article]
[img of red ball]

>So I'd like to engage AI risk from both these perspectives. I think the arguments for superintelligence are somewhat silly, and full of unwarranted assumptions.
>But even if you find them persuasive, there is something unpleasant about AI alarmism as a cultural phenomenon that should make us hesitate to take it seriously. 
>
This unpleasantry has grown even uglier in the following years, speading like a prion throughout Silicon Valley and a handful of presigious universities.

In part 2, I'll revisit each of Ceglowski's individual arguments against AI alarmism and see how they've stood the test of time.





## Cartography pt 2 links

VOX/FUTURE PERFECT -> EZRA KLEIN  / MATTY

CGP GREY NICK BOSTROM VIDEO -> Lifespan Extension Advocacy Foundation

 Lifespan Extension Advocacy Foundation ->SENS Research Foundation - OncoSENS, MitoSENS[8] and MitoMouse. 

80K hours AI listings:
Anthropic
ARC (Alignment Research Center)
OpenAI
Center for AI Safety
ARENA - https://www.arena.education/
Open Philanthropy
Center for a New American Security
FAR AI
Center on Long-Term Risk fka  Foundational Research Institute -> EA Foundation
AI Safety Hub
Centre for the Governance of AI fka Oxford’s Future of Humanity Institute
Redwood Research
Rethink Priorities
Future of Life Institute
Founders Pledge
Giving What We Can
Effective Altruism NYC
The Life You Can Save
EA Funds
Lightcone Infrastructure
Surviving and Flourishing -> collaborating with Oliver Habryka and others at Lightcone Infrastructure, Scott Garrabrant at the Machine Intelligence Research Institute, David Dalrymple at Protocol Labs, and several other interested parties. "concerned with the long-term survival and flourishing of sentient life."

https://survivalandflourishing.fund/s-process

Leverage account
https://medium.com/@zoecurzi/my-experience-with-leverage-research-17e96a8e540b
Emergent Venture awards

https://marginalrevolution.com/marginalrevolution/2019/11/progress-studies-tranche-of-emergent-ventures.html

https://forum.effectivealtruism.org/posts/eQKYhNvjSXGbivfLu/did-peter-thiel-give-the-keynote-address-at-an-ea-conference

leverage 2021 bottleneckss
https://docs.google.com/document/d/1vuvg9hhUiYJBprlSRv9BOzdEAMZD79n0O77l90UaS9c/edit#heading=h.p44yllv5rxdu
https://www.leverageresearch.org/bottlenecks

EA Foundation

EAF US

EAF Germany

EAF Switzerland -> Center on Long-Term Risk fka  Foundational Research Institute
EAF Switzerland -> Raising for Effective Giving (sponsors poker players https://reg-charity.org/)


ARENA - https://www.arena.education/
Kathryn O’Rourke - Operations Management -  King’s College London Effective Altruism uni group organizer
Callum McDougall - Cofounder & Curriculum Designer - https://www.perfectlynormal.co.uk/
His top three favorite things are " AI safety, Rationality, Sci-fi movies"

In a rare W move for anarcho-capitalists, Bet on It's Bryan Caplan collaborated with SMBC artist Zach Weinersmith to release a non-fiction in 2019 arguing in favor of open borders.

------