# We Need To Talk About Kurzgesagt (Again)


I'm far from the first person to raise questions regarding Kurzegast's funding. In January, YouTuber The Hated One released a video called [How Kurzgesagt Cooks Propaganda For Billionaires](https://www.youtube.com/watch?v=HjHMoNGqQTI) which has racked up 1.7 million views. Kurzgesagt gave an [official response](https://old.reddit.com/r/kurzgesagt/comments/10jlyyk/kurzgesagt_statement_to_the_conflict_of_interest/) on their subreddit, followed by a lengthy [counter-response](https://www.reddit.com/r/thehatedone/comments/10pb1q9/my_response_to_kurzgesagt/) on The Hated One's own sub. I don't want to re-hash all of that drama, so I'll be focusing this article on Kurzgesagt's behavior in 2023 after they responded to criticisms.

### Some Twitter Snooping

As part of The Hated One's counter-response, they had this to say about the idea of sponsor money influencing Kurzgesagt:

> I think Kurzgesagt perfectly aligns with the values of their sponsors. That doesn't make it immune to influence. It arguable makes it even worse. It is because of your values that you receive all this significant funding. The problem is, that any channel that would try to go against your values, would not receive such funding and would not have enough resources to compete with you.

After watching Kurzgesagt release increasingly EA-aligned videos, I fully agree with this sentiment.

To try and get an idea of where Kurzgesagt's own values lie, I turned to the tried and true method of checking who they follow on Twitter/X. Normally I would advise against reading deeply into the follows of a given account, but Kurzgesagt follow list is only at a meager 168 users.

As of this writing, Twitter allows me to see the 65 most recently followed accounts by @Kurz_gest, which is just over a third of his follows. These 65 accounts include the following EA-related people and groups:

* [Open Philanthropy](https://twitter.com/open_phil)
* [Tyler Cowen](https://twitter.com/tylercowen)
* [GiveWell](https://twitter.com/GiveWell)
* [Elon Musk](https://twitter.com/elonmusk)
* [Julia Galef](https://twitter.com/juliagalef), prominent Rationalist and co-founder of the Center for Applied Rationality
* [Effective Altruism](https://twitter.com/EffectvAltruism)'s official account, run by the Centre for Effective Altruism
* [80,000 Hours](https://twitter.com/80000Hours), prominent Effective Altruist job board advertised in Kurzgesagt's videos
* [Benjamin Todd](https://twitter.com/ben_j_todd), founder of 80,000 Hours
* [Michael Shellenberger](https://twitter.com/shellenberger), notable anti-progressive and downplayer of climate change
* [William MacAskill](https://twitter.com/open_phil), famous effective philosopher at the University of Oxford who coined the term "longtermism"
* [Toby Ord](https://twitter.com/tobyordoxford), another famous EA philosopher from Oxford who frequently collaborates with William McAskill
* [Liv Boeree](https://twitter.com/Liv_Boeree), an Effective Altruist YouTuber.
*  [Global Priorities Institute](https://twitter.com/GPIOxford), A longtermist research centre at the University of Oxford that was [funded](https://www.development.ox.ac.uk/news/new-global-priorities-institute-opens) by Open Philanthropy and wealthy Effective Altruist donor [Luke Ding](https://www.founderspledge.com/meet-the-team/luke-ding)
*  [Robin Hanson](https://twitter.com/robinhanson), economist, longtermist, futurist, cryonicist, and Rationalist who previously blogged with Elizer Yudkowsky



Samo Burja
@SamoBurja
There's never been an immortal society. Figuring out why. Founder of @bismarckanlys
. @longnow
fellow.


Julia Galef
@juliagalef
Author of THE SCOUT MINDSET and host of the Rationally Speaking podcast

Toby Ord
@tobyordoxford
Senior Research Fellow at Oxford University. Author â€” The Precipice: Existential Risk and the Future of Humanity

William MacAskill
@willmacaskill
Moral philosopher at Oxford. Author of Doing Good Better. New book, What We Owe The Future, is out now! http://tinyurl.com/2p98m49p Think long-term. Act now.
Centre for the Study of Existential Risk
@CSERCambridge
@Cambridge_Uni
interdisciplinary research centre dedicated to the study and mitigation of risks that could lead to human extinction or civilizational collapse.

80,000 Hours
@80000Hours
Free research and support to help graduates find careers tackling the world's most pressing problems. Newsletter & free book: http://80000hours.org/free-book/

### Grabby Aliens, Rational Animations, and SBF

In Kurzgesagt's video titled [The Second Deadliest Virus](https://www.youtube.com/watch?v=Kr57ax0OWMk), Kurzgesagt provides a link to a Rational Animations called [500 Million, But Not A Single One More](https://www.youtube.com/watch?v=ljmifo4Klss) in the video description, right before their disclosure of Open Philanthropy's funding. 500 Million, But Not A Single One More is an animated version of a [2020 thread of the same name](https://forum.effectivealtruism.org/posts/jk7A3NMdbxp65kcJJ/500-million-but-not-a-single-one-more) that was posted to the Effective Altruism Forum by a user named "jai". 

Rational Animations' video contains four separate links back to the EA forum, including the original thread. In the comments for that thread, a user excitedly points out that Kurzgesagt has indirectly promoted the thread, prompting jai to respond that he was "happy to see the animated-birds-explaining-things crossover with the animated-dogs-explaining-things, especially as I'm now working with the animated explainer dogs", indicating that he was now in an active partnership with Rational Animations.

In another thread called [Should Rational Animations invite viewers to read content on LessWrong?](https://www.lesswrong.com/posts/TNsLdAZmSzMHTeaKf/should-rational-animations-invite-viewers-to-read-content-on) posted in May, Rational Animations reveals that they have had discussion with Open Philanthropy about purposefully creating an "intellectual rabbit hole" leading to LessWrong for viewers to fall into.

> In my most recent communications with Open Phil, we discussed the fact that a YouTube video aimed at educating on a particular topic would be more effective if viewers had an easy way to fall into an "intellectual rabbit hole" to learn more. So a potential way to increase Rational Animations' impact is to increase the number of calls to action to read the sources of our videos. LessWrong and the EA Forum are good candidates for people to go learn more. 

Rational animation has also solicited [job applications](https://www.lesswrong.com/posts/mC5eWRwkfi3vJ9XB3/rational-animations-is-looking-for-an-ai-safety-scriptwriter) and [scripts relating to longtermism](https://www.lesswrong.com/posts/RH8nGG5vnuXc4eKu5/rational-animations-script-writing-contest) from LessWrong as well.

I'm not sure what kind of conversations Kurzgesagt has had with Open Philanthropy or if they found out about Rational Animations via their shared funding source. However, it's clear from Rational Animations' actions that they have an interest in using Open Philanthropy's donations to grow the Rationalist, EA, and longtermist movements and Kurzgesagt's promotion EA-related organizations seems to suggest that they're interested in doing the same.

### Centre for Existential Risk and AI Doom

 Dr. Kevin M. Esvelt from the Massachusetts Institute of Technology and     Dr. Cassidy Nelson from the University of Oxford.  Dr. Esvelt appears to be the primary source for this video, with Kurzgesagt claiming they based the script for the video on his book "Delay, Detect, Defend" and got additional fact checking from him.
 
Dr. Nelson, on the other hand, has no mention of her contributions or other qualifications in the field. Kurzgesagt doesn't mention for example, her current position as [Head of Biosecurity Police at the Centre for Long Term Resilience](https://www.longtermresilience.org/cassidy-nelson) or her previous position as a researcher at the Future of Humanity Institute. Both of these think tanks have direct ties to the EA movement and Dr. Nelson's [biography](https://futureoflife.org/person/cassidy-nelson/) on the Future of Life Institute's website shows that she was "convinced by long-termist arguments" to shift her career to focus on "x-risk mitigation". I'm not sure how or why Kurzgesagt found this particular biosecurity researcher, but it seems like a safe bet that it was thanks to their shared interest in longtermism.
 
[https://twitter.com/longresilience](Twitter bio) say that their focus is on "transforming resilience to extreme AI and biological risks". Kurzgesagt's video doesn't mention AI




### Live Boeree & The EA Poker Connection


Liv Boeree
@Liv_Boeree
Looking for the win-wins in life. Not a fan of Moloch traps. Brand new podcast out now, watch here:


Description
Welcome! I'm Liv - started out in astrophysics, ended up at the poker table. Now I make weird videos.

Things I love: game theory, physics and embracing uncertainty. 

Things I don't love: extinction risks and ideologues



