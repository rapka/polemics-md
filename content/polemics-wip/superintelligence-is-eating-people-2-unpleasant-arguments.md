# Superintelligence is Eating People, Part 2: Unpleasant Arguments

_This is part 2 of my modern analysis of Maciej Ceglowski's 2016 essay "Superintellegence: The Idea That Eats Smart People". Read part one 1 [here](link) and the original essay [here](https://idlewords.com/talks/superintelligence.htm).


Now that the reader is familiar with the premise of existential AI risk, Ceglowski lays out several arguments against taking the idea too seriously.

> So I'd like to engage AI risk from both these perspectives. I think the arguments for superintelligence are somewhat silly, and full of unwarranted assumptions.
> But even if you find them persuasive, there is something unpleasant about AI alarmism as a cultural phenomenon that should make us hesitate to take it seriously.
> First, let me engage the substance. Here are the arguments I have against Bostrom-style superintelligence as a risk to humanity...

With the benefit of 7 years' worth of hindsight, we can evaluate how these arguments have stood the test of time. 

### The Argument From Wooly Definitions

> The concept of "general intelligence" in AI is famously slippery. Depending on the context, it can mean human-like reasoning ability, or skill at AI design, or the ability to understand and model human behavior, or proficiency with language, or the capacity to make correct predictions about the future. 
> What I find particularly suspect is the idea that "intelligence" is like CPU speed, in that any sufficiently smart entity can emulate less intelligent beings (like its human creators) no matter how different their mental architecture. 

Because the definition of "general intelligence" is so slippery, many thinkers have fallen back to the most suspect definition of all: IQ.

This has manifested in the writings of noted [white supremacist](splc) Linda Gottfredson being cited as an "expert" on intelligence in _two separate AI papers_. Once wsas in 2008, in the *PhD these* of Google DeepMind co-founder shane Legg. The other was more recent, in the *introduction of Microsoft's infamous Sparks of AGI ChatGPT paper*.

> The Argument From Stephen Hawking's Cat
>
In summary:
> Stephen Hawking is one of the most brilliant people alive, but say he wants to get his cat into the cat carrier. How's he going to do it? 

I don't think there's anything in this argument that needs an update besides saying "RIP" to Stephen Hawking. In an unfortunate coindence however, Stephen Hawking is related to Elizer Yudkowsky by their [respective](hawking epstein link) [connections](propublica link) to Jeffery Epstein, who famously liked to invest in futurist ventures.

> The Argument From Einstein's Cat
> In summary:
> >So even an embodied AI might struggle to get us to do what it wants. 
> >
The Argument From Emus

In summary:
>We can strengthen this argument further. Even groups of humans using all their wiles and technology can find themselves stymied by less intelligent creatures. 

Likewise, these argument doesn't need any more elaboration. they holds up pretty well, and the story of the Emu War is still a hilarious example of mankind's hubris. 

The Argument From Slavic Pessimism

>We can't build anything right. We can't even build a secure webcam. So how are we supposed to solve ethics and code a moral fixed point for a recursively self-improving intelligence without fucking it up, in a situation where the proponents argue we only get one chance? 
>Time has shown that even code that has been heavily audited and used for years can harbor crippling errors. The idea that we can securely design the most complex system ever built, and have it remain secure through thousands of rounds of recursive self-modification, does not match our experience. 
>
>
I'm not slavic so I can't comment of whether this title is accurate but I do think the logic is sound, especially in light of much-hyped ChatGPT's tendency to hallucinate false information about every subject.

###. The Argument From Complex Motivations

>  It's very likely that the scary "paper clip maximizer" would spend all of its time writing poems about paper clips, or getting into flame wars on reddit/r/paperclip, rather than trying to destroy the universe. 
> If AdSense became sentient, it would upload itself into a self-driving car and go drive off a cliff. 
This is one of the more theoretical arguments about the nature of AI development, so it might not be as effective. Te phoenomenom he's describing is often seen in existing reinforcement learning systems, albeit simpler. Bots like Go's(link to ai flaws) will often have glaring flaws as a result of their lazer-like focus on a single goal. This naturally leads to...

### The Argument From Actual AI
In summary
>When we look at where AI is actually succeeding, it's not in complex, recursively self-improving algorithms. It's the result of pouring absolutely massive amounts of data into relatively simple neural networks. 
>
This is even more true in 2023 as it was in 2016. Models like ChatGPT or Bard are created by dumping virtually all of the internet into them and dumping even more manual curation on top of that. The latter is euphemistically referred to as "Reinforcement Learning from Human Feedback" which is simply a technical way of describing [underpaid human moderators](link to chatgpt sma). "Actual AI" in 2023 is often a smokescreen.

### The Argument From My Roommate
In summary:
> My roommate was the smartest person I ever met in my life. He was incredibly brilliant, and all he did was lie around and play World of Warcraft between bong rips. 
> It's perfectly possible an AI won't do much of anything, except use its powers of hyperpersuasion to get us to bring it brownies. 
This is a variation on the Argument for Complex Motivations. Like that argument, you may not find this reassuring at all if you're already worried about evil AI scenarios. All can do is speak for myself here, and my smart ass would much rather rip a bong and play video games than worry about these sci-fi AI scenarios. 

### The Argument From Brain Surgery
In summary:
> I can't point to the part of my brain that is "good at neurosurgery", operate on it, and by repeating the procedure make myself the greatest neurosurgeon that has ever lived.
> the hard takeoff scenario requires that there be a feature of the AI algorithm that can be repeatedly optimized to make the AI better at self-improvement. 
> 
Modern AI architectures don't really point to such a self-improvement ability. Even if it did exist,it would almost certainly run into VRAM issues on the GPUs powering modern AI data centers. As previously mentioned, much of AI's current behavior is simply obfuscated human labor.

### The Argument From Childhood
In summary:
> Even the smartest human being comes into the world helpless and crying, and requires years to get some kind of grip on themselves. 
> It's possible that the process could go faster for an AI, but it is not clear how much faster it could go. Exposure to real-world stimuli means observing things at time scales of seconds or longer. 

Like the previous argument, I would also add to this that this developmental period would certainly be noticed by whoever owns the computer hardware. Many of these extremely serious AI doomsday scenarios can simply be refuted by "turn off the computer".

### The Argument From Gilligan's Island

In summary:
> A recurring flaw in AI alarmism is that it treats intelligence as a property of individual minds, rather than recognizing that this capacity is distributed across our civilization and culture. 

Even in a scenario where these bots figured out how to co-ordinate across the internet, humans could easily notice and intercept these considering the scale of internet surveillance that already exists. "Turn off the computer" is back again.

