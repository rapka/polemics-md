# WTF is going with AI and effective altruism: a tl;dr explanation for people who aren't huge nerds

![Thinking emoji overlaid over Future Perfect's website](images/ea-tldr/ea-tldr.jpg)

Between and all the [weirdness]() going on at OpenAI, [statements from politicians]() about existential risk, and Sam Bankman-Fried's [recent trial](https://www.wired.com/story/sbf-trial-sam-bankman-fried-ftx-testimony/), there's been a lot of talk about the dangers of artificial intelligence, AI regulation and effective altruism lately. After trying and failing to explain this brain-melting rabbit hole to my friends, I felt the need to write a simplified explanation of just what the heck is going on with major AI companies. My hope is that people outside of the tech bubble can be spared the psychic damage of being exposed to our nonsense firsthand.

---

In the beginning, there was a group of science enthusiasts, authors, and AI researchers (some of whom worked at OpenAI and Google DeepMind). All of them read too much science fiction and began to get very worried that AI will one day become sentient, attain god-like powers, turn evil, and decide to exterminate the human race.

Meanwhile, a group of priviliged philosophers from Oxford and other fancy universities got together and said something like this to themselves:

> "Sure, philanthropy is pretty cool, but we can do it even better. We need to _tryhard_ at philanthropy. We need to **_min-max_** our charity donations for optimal effectiveness! Let's come up with new kinds of Ethical Math so we can objectively calculate the best charities to support and make the world the best place we can!" 

This idea sounds great in theory - I too love math, optimization, and helping people! Some of the early causes supported by the formulas are animal rights and disease prevention in Africa, both of which are cool with this African-American vegan. But outside of those successes, putting this system into practice very quickly became problematic, especially once AI got involved. Here's how:

1. Effective altruists (known as EAs) get to design the Ethical Math formulas and run the numbers all by themselves.
2. By definition, any project that the Ethical Math formulas endorse is automatically considered one of the Most Important Causes in the world. **This is great for the egos of everyone involved.**
3. Once the EAs plugged "Evil God AI" into their Ethical Math formulas, stopping this existential threat instantly becomes _the most important project in the history of mankind_. **This is great for the egos of everyone involved.** It's also great for the wallets of some of the people involved, especially when those people work in the field of AI Safety and are already friends with rich EAs like Sam Bankman-Fried. Unfortunately, it's not so great for people who are worried about real problems with AI happening today.
4. Next, effective altruists did some more Ethical Math and found that one of the most effective ways of spending money is by [recruiting more effective altruists](https://www.openphilanthropy.org/focus/ea-global-health-and-wellbeing/). The idea is that these new EAs will then spend their time and money on the most Effective causes while also Effectively recruiting more EAs. **This is great for the egos of everyone involved.**
5. Then, they took some future population projections and started imagining how their Ethical Math formulas will affect the world 10, 100, 1000, or even 10,000 years from now  and called it _Longtermism_. These projections make the Ethical Math make for stopping the Evil God AI even more important while also bringing into play other far-fetched ideas like colonizing space.
6. Because stopping Evil God AI is one of the Most Important Causes in history, EAs have started [lobbying governments](https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362) for strict regulations around large AI systems. **This is great for the bank accounts of the big companies currently trying to make Good God AI and annoying for their competitors.**

One side effect of this Ethical Math is that an Evil God AI killing **everyone** becomes a worse problem than climate change "just" killing or displacing millions. This is a problem for rich techies who can afford to run from climate change but don't think they can run from Evil God AI. Luckily for them, a Good God AI could potentially solve climate change with its superintelligence, which means that spending even more energy on AI is actually good for the planet. 

Another side effect caused by this rhetoric of saving humanity using math and technology is being **great for the egos of ambitious university students**. For example, the student-run Existential Risk Initiative club at Yale recently [rejected a prospective member](https://www.theatlantic.com/ideas/archive/2023/09/yale-college-undergrad-clubs-competitive/675219/) due to her inexperience and told to listen to more EA podcasts.

We're now in a position where a subset of people in this effective altruism/AI Safety network believe that God AI is going to exist very soon, possibly in the form of GPT. This has caused a schism within the movement about how the God AI can be controlled by "aligning it with human values", if at all. The pessimists within this group are currently freaking out about the end of the world while the optimists are excited for a new era of AI-guided humanity.

OpenAI is right in the middle this conflict as it started out as a non-profit before restructuring and accepting a multi-billion dollar investment from Microsoft in 2019, leading to [uncertainty](https://venturebeat.com/ai/as-anthropic-seeks-billions-to-take-on-openai-industrial-capture-is-nigh-or-is-it/) about the direction of the company with regards to AI doom - this uncertainty almost played into Altman's firing.

---

Well, there you have it folks. That's pretty much the shortest possible explanation of all the strangeness in Silicon Valley that's been in the news. Unfortunately, the press has not been great at covering this topic until fairly recently since they love scary headlines about dangerous AI.

After reading all this, one might come to the conclusion that the field of AI "existential risk" kinda seems like a pseudo-religion for developers that's tied up in a bunch corporate drama for clout and money. That assessment would be...correct.
